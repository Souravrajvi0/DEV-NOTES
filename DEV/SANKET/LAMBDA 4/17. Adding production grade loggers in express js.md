When your app is running on a customerâ€™s server or in the cloud, you canâ€™t just `console.log` and stare at the terminal. Instead:

### ğŸ” Why Logging Is Important

1. **Debugging in production**
    
    - If a customer calls saying _â€œThe page is blankâ€_ or _â€œPayment failedâ€_, you canâ€™t reproduce it instantly.
        
    - Logs tell you _what happened before the crash_ â†’ request path, user ID, DB query, error stack trace, etc.
        
2. **Tracing Issues Across Systems**
    
    - In microservices or distributed systems, one request may pass through multiple services.
        
    - Without logs, you wonâ€™t know _where_ the request failed. With logs, you can trace the journey.
        
3. **Performance Monitoring**
    
    - Logs can show slow queries, high memory usage, or bottlenecks.
        
    - Example: `DB query took 3.5s` â†’ you immediately know where to optimize.
        
4. **Security & Auditing**
    
    - Logs help detect suspicious behavior (e.g., repeated failed logins).
        
    - Companies often need logs for compliance (GDPR, HIPAA, etc.).

- **Logging** ensures developers/operators know whatâ€™s happening behind the scenes.

# ğŸŸ¢ What is Winston?

Winston is the most popular **logging library for Node.js**.  
It gives you **structured, level-based logging** (not just `console.log` spaghetti).

### Why not just `console.log`?

- `console.log` is **always the same level** (you canâ€™t tell if itâ€™s info, warning, or error).
    
- No log rotation, no saving to files.
    
- No timestamps by default.
    
- In production, raw logs get messy very quickly.
    

Winston solves this with:

1. **Log Levels** â†’ (error, warn, info, debug, etc.).
    
2. **Transports** â†’ Decide _where_ to log (console, file, database, external services).
    
3. **Formats** â†’ Pretty print, JSON, timestamps, colors.
    
4. **Custom loggers** â†’ Different loggers for different modules (auth, db, etc.).

What goes inside `createLogger()` in Winston.???

![image-539.png](../../Images/image-539.png)


![image-540.png](../../Images/image-540.png)



![image-541.png](../../Images/image-541.png)

![image-542.png](../../Images/image-542.png)


![image-543.png](../../Images/image-543.png)

---

## ğŸ”‘ What are Log Levels?

- **Log levels** are categories that indicate how important or severe a log message is.
    
- They help developers filter logs depending on the situation:
    
    - In **development**, you may want detailed logs.
        
    - In **production**, you usually want only warnings or errors.


---

## ğŸš¦ Common Log Levels (from lowest â†’ highest severity)

Most logging systems (like Winston, Bunyan, Pythonâ€™s `logging`, Javaâ€™s Log4j) follow similar conventions:

1. **DEBUG** ğŸ› ï¸
    
    - Very detailed information (used for debugging during development).
        
    - Example: â€œUser object after parsing request: {id: 12, name: 'Sourav'}â€
        
2. **INFO** â„¹ï¸
    
    - General information about the appâ€™s normal flow.
        
    - Example: â€œServer started on port 3000â€
        
3. **WARN / WARNING** âš ï¸
    
    - Something unexpected happened, but the app is still running fine.
        
    - Example: â€œAPI response time exceeded 2 secondsâ€
        
4. **ERROR** âŒ
    
    - A problem occurred that prevented some functionality.
        
    - Example: â€œDatabase connection failed: timeoutâ€
        
5. **FATAL / CRITICAL** ğŸ’€
    
    - Very severe errors that likely crash the app or require immediate attention.
        
    - Example: â€œOut of memory â€“ shutting downâ€
        

---

## ğŸ“Š How Theyâ€™re Used

- Each log message is tagged with a **level**.
    
- You can configure your logger to **capture only certain levels** depending on the environment:
    
    - Development â†’ `DEBUG` and above
        
    - Staging â†’ `INFO` and above
        
    - Production â†’ `WARN` and above
        

So if you set level = `WARN`, then only `WARN`, `ERROR`, and `FATAL` logs will appear.

---

âœ… In short: **log levels are filters that let you control the noise in your logs while still keeping the critical information.**


![image-537.png](../../Images/image-537.png)

# Step 3: Log Levels

Winston has **default levels** (similar to syslog):

|Level|Priority|Use Case|
|---|---|---|
|error|0|App crashed, DB connection failed|
|warn|1|Something unexpected, but app works|
|info|2|General info (server started, user logged in)|
|http|3|Track HTTP requests|
|verbose|4|Extra details|
|debug|5|For developers (debugging only)|
|silly|6|Rarely used|


ğŸ‘‰ **`level` in config decides the lowest priority that will be logged.**  
If you set `"info"`, then `"warn"` and `"error"` will log too (since theyâ€™re higher priority).

![image-538.png](../../Images/image-538.png)



![image-544.png](../../Images/image-544.png)

# Step 7: Advanced Stuff

- Different loggers for different modules (`authLogger`, `dbLogger`).
    
- Send logs to cloud services (AWS CloudWatch, LogDNA, Datadog).
    
- Log rotation with `winston-daily-rotate-file`.

âœ… So Winston replaces messy `console.log`s with **structured, multi-level, multi-destination logging**.


## ğŸ› ï¸ Where to Use `logger` in Your App

Once you create your `logger` with `createLogger()`,  
you can call methods like `logger.info()`, `logger.error()`, `logger.warn()`, etc. **anywhere in your code** where you want to record something.


![image-545.png](../../Images/image-545.png)


![image-546.png](../../Images/image-546.png)

ğŸ‘‰ So basically:

- You **create the logger once** (usually in a separate file like `logger.js`).
    
- Then you **import and use it anywhere** in your project instead of `console.log()`.

![image-547.png](../../Images/image-547.png)

![image-548.png](../../Images/image-548.png)


![image-549.png](../../Images/image-549.png)


ğŸ‘‰ _â€œWhat if some async/inner function runs thatâ€™s not directly tied to the request middleware â€” how do we keep the correlation id alive?â€_

This is a **real problem** in Node.js because of its async nature (callbacks, promises, setTimeout, etc.). Letâ€™s break it down:

---

## ğŸ” The Problem

- Middleware sets `req.correlationId` when an HTTP request comes in.
    
- You log inside the request handler â†’ âœ… correlation id is present.
    
- But deep inside:
    
    - Some async DB call
        
    - Or a queue consumer
        
    - Or a background job (not tied to HTTP request)
        

Your `req` object is gone â†’ âŒ correlation id lost.




###  **Async Context Propagation (Best Way)**

Node.js has a special API for this:  
ğŸ‘‰ **AsyncLocalStorage** (from `async_hooks` module).

It lets you store data (like correlation id) for the lifetime of an async chain, even across callbacks/promises.
![image-551.png](../../Images/image-551.png)

## 2. Solution: `AsyncLocalStorage`

`AsyncLocalStorage` (from the `async_hooks` module) lets you store data **per asynchronous execution chain**.  
That means if you set a value at the start of a request, you can access it **anywhere later in that async flow**, without passing it manually.


![image-552.png](../../Images/image-552.png)


### . The Core Issue in Node.js

- Node.js is **single-threaded** but **asynchronous**.
    
- That means your code is constantly â€œpausingâ€ and â€œresumingâ€ at different points (callbacks, promises, timers, DB queries).
    

ğŸ‘‰ Imagine you start handling **Request A**, then Node pauses it to handle **Request B**, then resumes A, then C, then B againâ€¦ all interleaved.

Now, if you want to keep **some piece of data tied to Request A** (like a correlation ID), you need a way to â€œcarry it alongâ€ no matter how many times the execution jumps between requests.




### ğŸ§© 2. Analogy

Think of a **backpack** ğŸ’:

- Each request gets its own backpack when it arrives.
    
- Everything that happens **in that requestâ€™s async flow** should have access to that backpack.
    
- Even if Node switches between requests, you donâ€™t lose the backpack contents.
    

`AsyncLocalStorage` = a system that **manages those backpacks** for you.  
So whenever you log, you can peek inside the current requestâ€™s backpack.



### 3. What It Actually Is

`AsyncLocalStorage` is a **class** from Nodeâ€™s `async_hooks` module that lets you:

- **Run code with a â€œstoreâ€ (context object)** attached.
    
- **Retrieve that store later**, anywhere in the same async chain.
    

That "store" can hold anything:

- `correlationId`
    
- `userId`
    
- `tenantId`
    
- `request start time`  
    etc.



![image-553.png](../../Images/image-553.png)


![image-554.png](../../Images/image-554.png)


ğŸ‘‰ `.run()` is how you **start a new context** and attach data to it.  
ğŸ‘‰ `.getStore()` is how you **read that context later** in async code.



## 1ï¸âƒ£ How to Generate (Make) a Correlation ID

A correlation ID is just a **unique identifier** per request (or per workflow).  
Most common â†’ **UUID v4** (random unique 128-bit string).


![image-555.png](../../Images/image-555.png)

![image-556.png](../../Images/image-556.png)

![image-557.png](../../Images/image-557.png)

![image-558.png](../../Images/image-558.png)
### Why does this happen fundamentally?

- The `req` object is **scoped only to the HTTP lifecycle**.
    
- Async/background jobs run **outside that lifecycle** (different tick of event loop, different worker process, sometimes different service).
    
- Without a mechanism like `AsyncLocalStorage` or explicit propagation, correlationId is lost.
    

---

âœ… Thatâ€™s why **attaching correlationId only to `req` works for simple synchronous request-response**, but fails in:

- Delayed async tasks (setTimeout, promises).
    
- Background workers (queues, cron jobs).
    
- Cross-service calls (HTTP requests to other microservices).


The reason we put the correlation ID in the **response header** (`res.setHeader("x-correlation-id", correlationId)`) is so that:

### 1. **Traceability for the client**

- When the client (browser, mobile app, or another microservice) gets the response, it can see **which correlation ID was used**.
    
- If something goes wrong, the client can send that ID back in bug reports, support tickets, or retry requests.
    
- Example:
    
    - Client: sends request â†’ gets response with `x-correlation-id: 1234`.
        
    - Later, user complains "This request failed with correlation ID 1234."
        
    - You search your logs for `1234` and immediately see the full request flow.
        

---

### 2. **Consistency across systems**

- In distributed systems (say, Service A â†’ Service B â†’ Service C), each service forwards the same correlation ID in request/response headers.
    
- This way, every system in the chain knows **itâ€™s the same request**, and you can trace it end-to-end.
    

---

### 3. **Debugging & Monitoring**

- Tools like Kibana, Elasticsearch, Datadog, or Grafana can automatically pick up correlation IDs from headers.
    
- This makes logs searchable by correlation ID, which is super useful for root-cause analysis.
    

---

âœ… In short: putting it in the **header** makes it **visible externally** (not just inside the server).  
If you only stored it in `req.correlationId`, only your server code would know. By setting it in headers, **both the client and any other services** can also carry it forward.

![image-559.png](../../Images/image-559.png)

![image-560.png](../../Images/image-560.png)



![image-561.png](../../Images/image-561.png)



![image-562.png](../../Images/image-562.png)

## ğŸ”¹ Why Daily Rotate?

- Normally Winston with `File` transport writes to one file (`app.log`).
    
- Over time â†’ file becomes huge (GBs).
    
- Instead: split logs by **date** (or size).
    
    - `app-2025-08-17.log`
        
    - `app-2025-08-18.log`
        
    - `app-2025-08-19.log`
        
- Old logs can auto-delete after X days.


![image-563.png](../../Images/image-563.png)


![image-564.png](../../Images/image-564.png)

### ğŸ” What happens here:

- `logs/` â†’ means Winston will create a folder named `logs` in your projectâ€™s root directory.
    
- Inside it, files like:
    
    `logs/app-2025-08-17.log logs/app-2025-08-18.log logs/app-2025-08-19.log`
    
- If the folder doesnâ€™t exist, you usually need to create it manually, otherwise Winston might throw an error.




## ğŸ§© Two Worlds of Logs

1. **Operational Logs (OLTP-style usage)**
    
    - Purpose: debugging, monitoring, tracing requests in real time.
        
    - Tools: Winston, ELK (Elasticsearch + Logstash + Kibana), Graylog, Datadog, Splunk.
        
    - Stored in: log storage/search systems (not typically OLAP DBs).
        
2. **Analytical Logs (OLAP-style usage)**
    
    - Purpose: find **patterns** in logs over time â†’ e.g.,
        
        - â€œWhat % of requests fail per day?â€
            
        - â€œWhich API endpoint is slowest?â€
            
        - â€œHow many users see error X by country?â€
            
    - This is **analytics** â†’ perfect for OLAP (Online Analytical Processing)

## ğŸ“Š So, Do We Put Logs in OLAP?

ğŸ‘‰ Usually, **not directly**. Logs are:

- high-volume,
    
- unstructured/semi-structured (JSON),
    
- very granular (millions per day).
    

Instead, the typical flow is:

1. **Generate logs** (with Winston, etc.).
    
2. **Ship them** to a log collector (Filebeat, Fluentd, Kafka, Logstash).
    
3. **Store them** in something that supports fast search:
    
    - Elasticsearch (most common),
        
    - OpenSearch,
        
    - Loki (Grafana).
        
4. **Aggregate + Analyze**
    
    - Either inside those tools (Kibana dashboards)
        
    - OR ETL (Extract-Transform-Load) logs â†’ OLAP DB (like ClickHouse, BigQuery, Snowflake) for deeper analysis.

## Why Not Just Dump Everything in OLAP?

- Logs are **too fine-grained** for OLAP directly.
    
- OLAP works better on **aggregated events** (e.g., â€œAPI latency avg per endpoint per hourâ€), not raw stack traces.
    
- Thatâ€™s why the pattern is:
    
    - Use **log stores (ELK/Datadog)** for debugging/tracing.
        
    - Use **OLAP DBs** for aggregated analytics (trends, patterns).


## Why Not Just Winston?

- Winston is only a **logger** â†’ it formats logs and writes them to destinations (console, file, DB, etc.).
    
- But Winston itself doesnâ€™t:
    
    - Index logs
        
    - Search logs
        
    - Provide dashboards or analytics
        
    - Handle **huge scale** (millions of logs per day)
        

So Winston is perfect for **creating logs**, but not for **managing logs** once they leave your app.

---

## ğŸ” Why Elasticsearch?

Elasticsearch is a **search + analytics engine** designed for massive, semi-structured data (like logs).

### Key Benefits:

1. **Fast search** ğŸ”
    
    - You can query:
        
        - â€œShow all ERROR logs in last 1hâ€
            
        - â€œFind all logs with correlationId=1234â€
            
    - Even if you have millions of logs, Elasticsearch indexes them for near real-time search.
        
2. **JSON-friendly** ğŸ§¾
    
    - Logs are often JSON (level, timestamp, correlationId, message).
        
    - Elasticsearch stores JSON natively, so you can query on any field.
        
3. **Scalable** âš¡
    
    - Handles billions of logs per day across clusters.
        
4. **Integration with Kibana** ğŸ“Š
    
    - Kibana (the dashboarding tool for Elasticsearch) lets you visualize logs:
        
        - Error trends
            
        - Latency histograms
            
        - API usage charts


![image-550.png](../../Images/image-550.png)


## ğŸ§© What is Amazon Redshift?

- **Redshift** = Amazonâ€™s **OLAP (Online Analytical Processing)** data warehouse.
    
- Optimized for:
    
    - Aggregations (SUM, AVG, COUNT)
        
    - Analytics across billions of rows
        
    - BI dashboards (Tableau, Power BI, Looker, QuickSight)
        

Not optimized for:

- Real-time search
    
- Debugging individual log lines
    
- Handling high-frequency inserts (like raw logs every second)
    

---

## ğŸ” Elasticsearch vs Redshift for Logs

|Feature|Elasticsearch ğŸŸ¡|Redshift ğŸ”´|
|---|---|---|
|**Best for**|Search & filtering logs (real-time debugging)|Aggregated analytics (trends, BI)|
|**Data type**|Semi-structured JSON|Structured tables (columnar storage)|
|**Query style**|`level:error AND correlationId=1234`|`SELECT COUNT(*) FROM logs WHERE level='error'`|
|**Use case**|â€œWhat happened in request 1234?â€|â€œHow many errors per service per day?â€|
|**Latency**|Real-time (seconds)|Batch (minutesâ€“hours)|
|**Scaling**|Horizontally across nodes|Columnar, suited for TBâ€“PB scale|

---

## ğŸ“Š Logs + Redshift Flow

You usually **donâ€™t dump raw Winston logs directly into Redshift**.  
Instead, the pattern is:

1. **App logs (Winston)** â†’ JSON
    
2. **Log pipeline (S3, Kafka, Firehose, etc.)** â†’ Collect/store raw logs
    
3. **ETL process** (Glue, Spark, dbt, etc.) â†’ Transform logs
    
    - Example: convert from raw JSON into structured tables
        
    - Fields: timestamp, level, service, correlationId, latency, error_type
        
4. **Load into Redshift**
    
    - Now you can query:
        
        `SELECT service, COUNT(*) AS error_count FROM logs WHERE level = 'error' GROUP BY service;`


