
# ğŸ§± Monolith Architecture

### 1. What is a Monolith?

A **monolith** is a software architecture style where the **entire application is built as a single unit**.

- All the features (user login, payments, search, reviews, etc.) are part of **one codebase**.
    
- It usually runs as **one executable / one binary / one service**.
    
- If you deploy, you deploy **everything together** (even if you changed only one small feature).
    

Think of it as a **big building made of one block** â†’ everything is tightly connected.

---

### 2. Example of Monolith

- Imagine an **e-commerce app**.
    
    - Login
        
    - Product catalog
        
    - Cart
        
    - Orders
        
    - Payments
        

In a monolith, **all of these live inside one codebase**, one big server.  
If the payments code crashes, it might affect the whole app.

---

### 3. Characteristics of a Monolith

- **One codebase, one repo** (not always, but usually).
    
- **One binary/executable** â†’ you build & deploy the whole thing at once.
    
- **Shared database** â†’ usually one DB serves everything.
    
- **Tight coupling** â†’ modules are dependent on each other.
    

ğŸ‘‰ Example: If your monolith is built with **Ruby on Rails**, the whole app (catalog, payments, orders, etc.) lives in one Rails app.

---

### 4. Pros of Monolith

âœ… **Simple to start** â€“ easy for small teams, fast to build.  
âœ… **Easier debugging** â€“ everything in one place.  
âœ… **Performance** â€“ calls between modules are function calls, not network calls.  
âœ… **Less DevOps overhead** â€“ just deploy one service.

---

### 5. Cons of Monolith

âŒ **Scaling issues** â€“ you canâ€™t scale just payments or catalog; you must scale the whole app.  
âŒ **Slower deployments** â€“ even small changes require full redeploy.  
âŒ **Hard to understand** â€“ over time, codebase becomes huge.  
âŒ **Team bottlenecks** â€“ large teams stepping on each otherâ€™s code.  
âŒ **Tech lock-in** â€“ you canâ€™t use different languages for different modules (all must be in same tech stack).

---

### 6. Real-world Examples

- **Stack Overflow** â†’ still a monolith (built in ASP.NET MVC + SQL Server).
    
    - They keep it super optimized with caching, strong DB design, and careful scaling.
        
    - Works fine because their workload is mostly **reads (Q&A viewing)** which can be cached.
        
- **E-commerce apps** â†’ often **start as monoliths**, but face problems:
    
    - Flash sales â†’ payments slow down â†’ whole site crashes.
        
    - Catalog update â†’ deploy takes too long.
        
    - Different teams (payments vs catalog vs orders) interfere with each otherâ€™s code.
        
    - Thatâ€™s why companies like Amazon, Flipkart, etc. **moved to microservices**.
        

---

### 7. What makes a monolith a "monolith"?

Itâ€™s not just â€œall code in one repo.â€  
A true monolith has:

- **Single deployable unit (binary/executable/service)**
    
- **Single runtime** â†’ everything runs in one process (not split by network).
    
- **Shared database** (usually one schema for everything).
    

So yes â†’ repo is part of it, but the main thing is **one deployable + one runtime**.

---

âœ… **Use case / Connectivity:**  
Monoliths are great when:

- Youâ€™re building an MVP or small app.
    
- Team size is small.
    
- You want **speed** over scalability.
    

But as user base grows (like 100K concurrent users in contests), monoliths start to **break under scale**, which leads to microservices.



# ğŸ§© Microservices Architecture

### 1. What are Microservices?

Microservices is an architectural style where the application is broken down into **smaller, independent services**.

Each service:

- Has **its own codebase**.
    
- Runs as a **separate process** (usually in its own container like Docker).
    
- Communicates with other services through **network calls (HTTP/REST, gRPC, message queues, etc.)**.
    
- Can be built, deployed, and scaled **independently**.
    

ğŸ‘‰ Think of it like a **city**:

- Monolith = one giant building.
    
- Microservices = many smaller buildings (banks, stores, hospitals) working together.
    

---

### 2. Example (E-commerce App)

- **Users Service** â†’ handles login, signup, authentication.
    
- **Catalog Service** â†’ manages product listing.
    
- **Orders Service** â†’ manages orders.
    
- **Payments Service** â†’ handles payments.
    
- **Reviews Service** â†’ handles customer reviews.
    

Each of these runs separately and communicates via APIs.

If you want to scale payments during a **Black Friday sale**, you scale only the **Payments Service** (not the entire app).

---

### 3. Characteristics of Microservices

- **Decoupled services** â†’ each does one job well.
    
- **Own database (sometimes)** â†’ services donâ€™t always share the same DB. (ex: Payments might use PostgreSQL, Catalog might use MongoDB).
    
- **Independent deployment** â†’ teams deploy without waiting on others.
    
- **Polyglot** â†’ different services can use different languages (Node.js, Go, Python, etc.).
    

---

### 4. Pros of Microservices

âœ… **Scalability** â€“ scale only whatâ€™s needed.  
âœ… **Faster deployments** â€“ teams work independently.  
âœ… **Resilience** â€“ if reviews service crashes, orders still work.  
âœ… **Flexibility** â€“ use best tech for each service.  
âœ… **Team autonomy** â€“ different teams own different services.

---

### 5. Cons of Microservices

âŒ **Complexity** â€“ more services â†’ more networking, monitoring, logging needed.  
âŒ **Latency** â€“ network calls are slower than function calls.  
âŒ **Data consistency** â€“ harder because data is spread across services.  
âŒ **DevOps overhead** â€“ need CI/CD pipelines, container orchestration (Kubernetes), service discovery, etc.

---

### 6. Real-world Examples

- **Amazon** â†’ originally a monolith, now microservices. Each service has its own API.
    
- **Netflix** â†’ thousands of microservices (streaming, recommendations, billing, etc.).
    
- **Uber** â†’ started as a monolith, migrated to microservices to handle surge pricing, trips, maps, payments separately.
    

---

### 7. Monolith vs Microservices

|Feature|Monolith|Microservices|
|---|---|---|
|**Codebase**|One big codebase|Many small codebases|
|**Deployment**|One binary / one service|Each service deployed independently|
|**Scaling**|Scale whole app|Scale only needed service|
|**Database**|Usually one DB|Each service may have own DB|
|**Communication**|Function calls (fast)|API/Network calls (slower)|
|**Team Structure**|Hard to split|Teams own independent services|
|**Best for**|Small apps, MVPs|Large-scale apps, complex domains|

---

âœ… **Use case / Connectivity**:

- **InterviewBit CodeAgon** â†’ stayed monolith but had to optimize DB + caching for 100K concurrent users. If they had microservices, they could scale specific services like _submissions_ or _leaderboard_.
    
- **E-commerce apps** â†’ often move to microservices because payments, orders, catalog need to scale independently.
    
- **Stack Overflow** â†’ still monolith but heavily optimized because their domain is simpler (Q&A + voting).


# ğŸ“‚ Monorepo

### 1. What is a Monorepo?

A **monorepo (monolithic repository)** is a version control strategy where **multiple projects/services live inside a single repository**.

- It does **not** mean monolith architecture.
    
- You can have **microservices** but still put all of them in one repo.
    
- Monorepo is about **code organization**, not runtime architecture.
    

ğŸ‘‰ Think of it like having **all departments of a company in one building** (HR, Sales, Tech, Finance) â†’ still separate, but managed together.

---

### 2. Example

- Suppose you build an e-commerce system:
    
    - `users-service/`
        
    - `catalog-service/`
        
    - `orders-service/`
        
    - `payments-service/`
        
    - `reviews-service/`
        

All of these live inside **one Git repo**.  
Each service has its own folder, maybe its own package.json (if Node.js) or build config.

---

### 3. Why use a Monorepo?

- **Single source of truth** â†’ all code in one place.
    
- **Easy dependency sharing** â†’ common libraries (like logging, auth utils) shared easily.
    
- **Consistent tooling** â†’ same linting, CI/CD, code quality rules.
    
- **Atomic commits** â†’ if you change API in one service, you can also update the client service in the same commit.
    

---

### 4. Problems with Monorepo

âŒ **Scalability** â†’ repo can become huge (Googleâ€™s monorepo has billions of lines of code).  
âŒ **Build times** â†’ need advanced tooling (Bazel, Nx, Turborepo) to avoid rebuilding everything.  
âŒ **Permissions** â†’ harder to give teams access to only their code.

---

### 5. Real-world Examples

- **Google** â†’ uses a massive monorepo with special tooling (Bazel).
    
- **Facebook** â†’ also uses monorepo for consistency.
    
- **Uber** â†’ started with monorepo but moved parts away because repo got too big.
    

---

### 6. Tooling for Monorepo

- **Nx** (JS/TS world, good for microservices + frontend apps).
    
- **Turborepo** (modern JS/TS monorepo tool by Vercel).
    
- **Bazel** (used by Google, very scalable).
    
- **Lerna** (older, for managing npm packages in monorepo).
    

---

âœ… **Use Case / Connectivity**

- If youâ€™re a startup â†’ monorepo helps because all code is together, easier to coordinate.
    
- If youâ€™re big like Google â†’ monorepo can still work, but you need very advanced tooling.
    
- If youâ€™re Amazon â†’ you might prefer **polyrepo** (each team owns its repo) because services are too large and independent.
    

---

âš¡ So: **Monorepo â‰  Monolith**.  
You can have **microservices inside a monorepo**. The repo structure is just about **how you organize code**, not how you run the system.


# âš¡ Turborepo

### 1. What is Turborepo?

- **Turborepo** is a **monorepo build system** for JavaScript/TypeScript projects.
    
- It was created by **Vercel** (the company behind Next.js).
    
- Its goal is to make **monorepos fast, efficient, and developer-friendly**.
    

ğŸ‘‰ In simple terms: Turborepo is a **tool that helps manage multiple apps/packages inside one repo** without slowing down builds & deployments.

---

### 2. Why do we need Turborepo?

Monorepos can get **slow and messy** because:

- When you change one file, the whole repo might rebuild.
    
- Running tests/lint across all projects is slow.
    
- Deploying independent apps from the same repo is tricky.
    

Turborepo solves these by:

- **Caching** â†’ remembers past builds/tests, reuses results.
    
- **Task pipelines** â†’ only runs affected tasks (not everything).
    
- **Remote cache** â†’ multiple devs share cache (CI/CD builds get faster).
    
- **Output sharing** â†’ services can depend on each other and still build efficiently.



![image-707.png](../../Images/image-707.png)

### 4. Features of Turborepo

- **Incremental builds** â†’ only rebuilds changed parts.
    
- **Shared cache** â†’ across devs and CI.
    
- **Parallel execution** â†’ tasks run fast.
    
- **Remote caching** â†’ Vercel offers cloud cache.
    
- **Works with multiple package managers** (npm, yarn, pnpm).
    

---

### 5. Turborepo vs Monorepo

- Monorepo is just a **code organization style** (all code in one repo).
    
- Turborepo is a **tool** to make monorepos manageable, especially in **JS/TS ecosystems**.
    

---

### 6. Real-world Usage

- **Vercel** â†’ uses Turborepo internally for Next.js and other products.
    
- **Startups** â†’ use Turborepo to manage frontend (React, Next.js) + backend (Node.js) + shared libraries.
    
- **Big companies** â†’ might prefer **Bazel** or custom tooling, but Turborepo is great for web-focused teams.
    

---

âœ… **Use case / Connectivity**

- If youâ€™re building an **e-commerce app with microservices** (frontend + backend + shared UI + utilities), Turborepo helps keep everything in **one repo with efficient builds**.
    
- This avoids duplication (donâ€™t rebuild everything every time) and speeds up development.
    
- Itâ€™s like giving your **monorepo a turbo engine** ğŸš—ğŸ’¨



# âš–ï¸ Scaling (Servers & Databases)

This is a huge one, so Iâ€™ll carefully break it down into **vertical scaling, horizontal scaling, autoscaling, warm instances**, and finally connect it to **servers vs databases**.

Scaling = handling **more load (users, requests, data)** by **adding more resources**.

Example: If your app runs fine for 1,000 users but struggles at 100,000, you need to **scale up**.

---

## 2. Two Types of Scaling

### ğŸ—ï¸ (a) Vertical Scaling (Scale Up)

- Add **more power to one machine** (bigger CPU, more RAM, faster SSD).
    
- Like upgrading from a 2BHK apartment â†’ 4BHK.
    
- Easy to do (change machine type in AWS/GCP).
    

âœ… Pros:

- Simple, no code changes.
    
- Works well up to a limit.
    

âŒ Cons:

- Thereâ€™s a maximum size a single machine can go.
    
- Expensive at high end.
    
- Single point of failure.
    

---

### ğŸŒ (b) Horizontal Scaling (Scale Out)

- Add **more machines/instances** and distribute load.
    
- Example: Instead of 1 huge server, run 10 smaller ones behind a **load balancer**.
    
- Like opening multiple branches of a restaurant.
    

âœ… Pros:

- Can scale infinitely (add more servers).
    
- No single point of failure (if done right).
    
- Cost-effective compared to one giant server.
    

âŒ Cons:

- Harder to manage (need load balancer, session handling, DB distribution).
    
- Services must be stateless (e.g., canâ€™t store user sessions in memory).


## 3. Autoscaling

- Cloud providers (AWS, GCP, Azure) let you **automatically add/remove machines** based on load.
    
- Example: During CodeAgon contest â†’ traffic spikes â†’ autoscaling group adds 20 more servers. After contest â†’ traffic drops â†’ servers shut down to save money.
    

---

## 4. Warm Instances ğŸ”¥

- **Problem**: When autoscaling spins up new servers, they take time to start (cold start).
    
- **Solution**: Keep a few â€œwarm instancesâ€ running â†’ not serving traffic, but ready to join immediately.
    
- Like keeping extra delivery bikes idle outside a restaurant during lunch rush.



## 5. Scaling Databases vs Servers

Servers are easy to scale horizontally (just add more stateless app servers).

Databases are harder:

- **Vertical scaling** â†’ add CPU/RAM to DB server (easy but limited).
    
- **Horizontal scaling** â†’
    
    - Replication (read replicas â†’ scale reads).
        
    - Sharding (split data across multiple DBs â†’ scale writes).
        
    - Caching layer (Redis, Memcached) to reduce DB pressure.


## 6. Real-world Examples

- **Facebook** â†’ scales horizontally with thousands of servers + caching (Memcached).
    
- **Uber** â†’ autoscaling microservices with warm instances (because rides demand fluctuates).
    
- **LinkedIn** â†’ heavy DB sharding + caching to handle profiles & connections.
    
- **InterviewBit (CodeAgon)** â†’ contest traffic spikes â†’ needed autoscaling + DB optimization to handle 100K concurrent users.
    

---

âœ… **Use case / Connectivity**

- Scaling is about **cost vs performance tradeoff**.
    
- For a monolith â†’ scaling is harder because everything is tied together.
    
- For microservices â†’ easier, since you can scale only whatâ€™s needed (payments, submissions, etc.).
    
- Warm instances + autoscaling help survive sudden peaks (like contests or flash sales).


### Why scaling is harder?

Imagine your app has three parts:

1. **Authentication** (users logging in)
    
2. **Search** (users searching for products)
    
3. **Payments** (users buying products)
    

In a **monolith**, these three are tied together in one big process. If suddenly the **search** feature is getting heavy load (say millions of searches), but payments and auth are not, you cannot just â€œscale search.â€

â¡ï¸ You have to run **more copies of the entire monolith** (auth + search + payments together).

- This wastes resources (youâ€™re also scaling payments & auth even though they donâ€™t need it).
    
- Makes deployments heavier (because every new version touches the entire app).
    
- Debugging becomes harder (logs of everything are mixed).
    
- Database might become a single bottleneck since all features are hitting the same DB.
    

---

### Example

Suppose Amazon was a monolith:

- **Black Friday sale** â†’ search & product catalog need huge scaling.
    
- But payments & user profile donâ€™t need that much load.
    

Still, you would have to **scale the whole Amazon monolith** just to handle search traffic â†’ inefficient.

---

### In contrast, with **microservices**:

- Each feature is separated.
    
- You can scale only the â€œsearch serviceâ€ while leaving payments/auth as they are.
    
- More efficient, but comes with complexity of managing many services.
    

---

ğŸ‘‰ So when people say:

> â€œScaling a monolith is harder because everything is tied togetherâ€

They mean you **canâ€™t scale parts independently** â€” you can only scale the whole thing



## ğŸ— Ways to Scale a Monolith

### 1. **Vertical Scaling (Scale Up)**

- Run the monolith on a **bigger machine**: more CPU, more RAM, faster disks.
    
- Easiest way but has a **limit** (and can get expensive).
    

Example: Your Node.js/Java monolith runs fine on 8 cores â†’ you move it to a 32-core server.

---

### 2. **Horizontal Scaling (Scale Out)**

- Run **multiple copies of the same monolith** behind a **load balancer**.
    
- Each copy handles some part of the incoming traffic.
    
- Requires your app to be **stateless** (e.g., donâ€™t keep user session in memory â†’ use Redis or DB instead).
    

Example:

- 10,000 users hitting your app.
    
- You deploy **10 monolith instances**.
    
- Load balancer distributes requests among them.
    

---

### 3. **Database Scaling**

- Since monoliths usually share **one big database**, thatâ€™s often the bottleneck.
    
- Strategies:
    
    - **Indexes & query optimization** (tuning queries).
        
    - **Read replicas** (one master DB for writes, multiple replicas for reads).
        
    - **Sharding** (split data across multiple DBs by user_id, region, etc.).
        
    - **Caching** (Redis, Memcached) to avoid hitting DB for every request.
        

---

### 4. **Caching**

- Cache heavy requests (product catalog, search results, user profiles) so not every request hits the DB.
    
- Can be **in-memory (Redis/Memcached)** or **edge caches (CDN)**.
    

---

### 5. **Job Queues / Async Processing**

- Offload heavy tasks (sending emails, generating reports, resizing images) to background workers.
    
- Keeps the main monolith fast and responsive.
    

---

### 6. **Code Splitting Inside Monolith** (Modularization)

- Even though still one deployable unit, you can make it **modular internally**.
    
- E.g., separate packages/libraries for auth, payments, orders â†’ reduces coupling.
    
- Makes it easier to eventually migrate parts to microservices if needed.
    

---

âœ… So you _can_ scale a monolith to handle **millions of users**. The tradeoff is:

- Scaling is **all-or-nothing** (canâ€™t just scale search separately).
    
- Eventually youâ€™ll hit limits of complexity â†’ thatâ€™s when companies move to **microservices**.


# ğŸ‘¥ Load & Concurrent Users

### 1. What does â€œConcurrent Usersâ€ mean?

- **Concurrent users** = number of users actively **using the system at the same time**.
    
- Example:
    
    - 1M people registered for a contest.
        
    - But maybe **100K are solving/submitting simultaneously** â†’ thatâ€™s 100K concurrent users.
        

ğŸ‘‰ Key: Itâ€™s not total users, but those **active in parallel**.

---

### 2. Why does concurrency matter?

- Servers & databases donâ€™t care about total users.
    
- They care about **how many requests per second (RPS)** they must handle at the same time.
    
- If your app is built for 5K concurrent users but contest spikes to 100K, system crashes.
    

---

### 3. Metrics to Track in High Concurrency

- **Latency** â†’ time to respond (e.g., keep <150ms for submissions).
    
- **Throughput (RPS)** â†’ requests per second your system can process.
    
- **Error rate** â†’ % of failed requests when overloaded.
    
- **Peak load** â†’ maximum traffic spike you must survive.



### 4. Example: InterviewBit CodeAgon (Case Study)

- Problem: Monolith system â†’ but needed to support **100K concurrent users** for coding contest.
    
- In 4 weeks, they:
    
    1. **Optimized DB** (indexes, bulk inserts, read replicas).
        
    2. **Added caching layer (Redis)** to avoid DB overload.
        
    3. **Autoscaling app servers (EC2 + load balancer)** to handle traffic.
        
    4. **Load tested** before contest to simulate 100K submissions.
        
    5. **Kept warm instances** so servers could join instantly at contest start.
        

ğŸ‘‰ Result: Submissions processed <150ms, even with 100K users online.

---

### 5. How Companies Handle Concurrency

- **Facebook** â†’ billions of concurrent users â†’ massive caching + CDN + DB sharding.
    
- **WhatsApp** â†’ 2B concurrent users, but each server handles millions because of Erlang (lightweight concurrency model).
    
- **Uber** â†’ fluctuating concurrency (rush hours) â†’ autoscaling + warm instances.
    
- **Stack Overflow** â†’ lower concurrency (not real-time), but still uses aggressive caching + optimized SQL queries to serve millions of page views.


### 6. Analogy

Imagine a **restaurant**:

- Total users = 10,000 customers in a day.
    
- Concurrent users = 200 eating lunch **at the same time**.
    
- If you only have 100 chairs (capacity = 100 concurrent), then 100 people must wait (queue = latency).
    

---

âœ… **Use case / Connectivity**

- Concurrency decides **infrastructure size**.
    
- Monoliths can handle high concurrency if **optimized** (Stack Overflow is proof).
    
- But contests/e-commerce sales need **autoscaling + caching + DB tuning** to handle sudden concurrency peaks.


# ğŸ—„ï¸ Database Optimization

When 100K concurrent users hit your system, the **database becomes the bottleneck**. Optimizing it is critical.

---

## 1. Why DB is a Bottleneck?

- Servers can scale horizontally (add more instances).
    
- But DBs are harder to scale, because:
    
    - They store **stateful data**.
        
    - Writes must be **consistent**.
        
    - Scaling writes = complex (sharding, replication).
        

So we **optimize DB first before scaling**.

---

## 2. Core Optimization Techniques

### ğŸ” (a) Indexes

- An **index** is like the index of a book â†’ speeds up search.
    
- Without index â†’ DB scans entire table.
    
- With index â†’ DB jumps directly to relevant rows.
    

ğŸ‘‰ Example:

- Table `Submissions(userId, problemId, status)`
    
- If you frequently query `WHERE userId = ?`, create an index on `userId`.
    

âœ… Faster reads.  
âŒ Slower writes (because index must also be updated).


### ğŸ“Š (b) Query Optimization

- Write **efficient SQL queries**.
    
- Avoid `SELECT *` (fetch only needed columns).
    
- Use **joins carefully** (denormalize if joins are too costly).
    
- Use **LIMIT, pagination** instead of fetching all.
    

ğŸ‘‰ Example: Contest leaderboard â†’ use `LIMIT 100` instead of fetching 100K results.

---

### ğŸ“¦ (c) Bulk Writes / Batch Processing

- Instead of inserting one row per request â†’ batch multiple inserts in one query.
    
- Example:
    
    - Bad â†’ insert 100 submissions â†’ 100 DB calls.
        
    - Good â†’ insert 100 submissions in one bulk insert â†’ 1 DB call.
        

âœ… Reduces DB load significantly.


### âš¡ (d) Connection Pooling

- Databases canâ€™t handle unlimited connections.
    
- Use a **connection pool** â†’ reuses existing connections instead of opening new ones each time.
    

---

### ğŸ§° (e) Caching Layer

- Use **Redis/Memcached** for frequently accessed data.
    
- Example: Contest leaderboard cached â†’ donâ€™t query DB every second.
    

---

### ğŸ—‚ï¸ (f) Read Replicas & Scaling

- **Read replicas** â†’ copy of DB used only for read queries.
    
    - Master handles writes.
        
    - Replicas handle reads (leaderboards, profiles, etc.).
        
- **Sharding** â†’ split data across multiple DBs (e.g., by userId range).
    
    - Users 1â€“1M in DB1, users 1Mâ€“2M in DB2.
        
    - Helps scale writes.
        

---

## 3. Tools for DB Optimization

- **Explain query plan** â†’ see how DB executes your query.
    
- **Indexes monitoring** â†’ drop unused indexes.
    
- **Partitioning** large tables (by date, by range).
    

---

## 4. Real-world Examples

- **Stack Overflow** â†’ still monolith but super optimized DB with:
    
    - Heavy indexing.
        
    - Caching (Redis + in-memory caching).
        
    - Denormalized tables for fast queries.
        
- **Facebook** â†’ MySQL sharded into thousands of DBs.
    
- **Uber** â†’ Mix of MySQL + Cassandra (distributed DB).
    
- **CodeAgon** â†’ optimized submissions DB with indexes, bulk inserts, caching, and read replicas to survive 100K concurrency.
    

---

âœ… **Use case / Connectivity**

- DB optimization is the **first step before scaling infra**.
    
- If you donâ€™t optimize queries/indexes, scaling servers wonâ€™t help (theyâ€™ll still overwhelm the DB).
    
- Contest/e-commerce scenarios rely heavily on **caching + bulk writes + read replicas** to survive spikes.


.

---

# ğŸ§ª Load Testing

### 1. What is Load Testing?

Load Testing = **simulating real-world traffic** on your system before actual users arrive.

- Goal: Check **how your app behaves under expected or peak load**.
    
- Itâ€™s like a **mock exam** before the real contest.
    

ğŸ‘‰ Example: CodeAgon expects **100K concurrent users** â†’ simulate 100K fake users submitting code and see if servers & DB hold up.

---

### 2. Why Load Testing is Important?

- Prevents **surprises in production**.
    
- Finds **bottlenecks** (DB, CPU, network, cache).
    
- Helps decide **infra capacity** (how many servers needed?).
    
- Ensures **SLAs** (e.g., submissions must be <150ms).
    

---

### 3. Key Types of Load Testing

- **Load Testing** â†’ normal expected traffic (e.g., 50K users).
    
- **Stress Testing** â†’ push beyond limits (e.g., 200K users) â†’ see where system breaks.
    
- **Spike Testing** â†’ sudden jump in traffic (e.g., from 1K â†’ 100K in 1 min).
    
- **Soak Testing** â†’ run high load for long time (e.g., 24 hours) â†’ check memory leaks, stability.
    

---

### 4. Metrics Collected

- **Response time (latency)** â†’ how fast requests return.
    
- **Throughput (RPS/QPS)** â†’ how many requests handled per sec.
    
- **Error rate** â†’ failed requests under load.
    
- **Resource usage** â†’ CPU, RAM, DB connections.
    

---

### 5. Tools for Load Testing

- **JMeter** (Apache, Java-based, widely used).
    
- **Locust** (Python-based, easy to write user flows).
    
- **k6** (modern, JS-based, great for CI/CD).
    
- **Gatling** (Scala-based, high performance).
    

ğŸ‘‰ Example: Locust can simulate 100K fake users logging in, browsing catalog, submitting answers, etc.

---

### 6. Real-world Examples

- **InterviewBit CodeAgon** â†’ ran load tests before contest to ensure DB + servers survive 100K concurrency.
    
- **Facebook** â†’ stress tests new features at 2â€“3x expected load before rollout.
    
- **Uber** â†’ spike testing during New Yearâ€™s Eve (crazy surge).
    
- **LinkedIn** â†’ soak tests messaging to ensure no memory leaks when billions of messages are exchanged daily.
    
- **Microsoft** â†’ load tests Office 365 with simulated corporate traffic before major releases.
    

---

### 7. Analogy

Imagine youâ€™re opening a new **stadium**:

- You donâ€™t wait for real match day.
    
- You first invite 100K fake volunteers to enter, use toilets, buy food â†’ check if stadium survives.
    

Thatâ€™s **load testing**.

---

âœ… **Use case / Connectivity**

- Load testing ensures youâ€™re **ready for peak traffic**.
    
- It guides **how many servers, what DB optimizations, how much caching** you need.
    
- Without it, contests/e-commerce flash sales crash under real users

# ğŸ¢ Real-world Examples & Anecdotes

### 1. **Stack Overflow (Monolith at Scale)**

- Still runs on a **monolithic .NET architecture**.
    
- Why? Their workload is **read-heavy** (mostly Q&A pages being read).
    
- They scale by:
    
    - **Aggressive caching** (Redis + CDN, so same questions donâ€™t hit DB repeatedly).
        
    - **Optimized SQL queries** (lots of DB tuning).
        
    - **Vertical scaling**: very powerful DB servers.
        
- Proves: A monolith **can work** if carefully optimized and workload is predictable.
    

---

### 2. **Uber (Microservices + Load Testing)**

- Uber started as a **monolith**, but as it grew â†’ moved to **thousands of microservices**.
    
- Why? Ride-matching, payments, notifications, surge pricing all need to scale independently.
    
- They use:
    
    - **Load testing** before New Yearâ€™s Eve (crazy peak).
        
    - **Warm instances + autoscaling** to handle sudden surge.
        
    - **Monorepo â†’ Polyrepo** transition: each microservice has its own repo.
        
- Lesson: When complexity grows, **microservices help teams move faster**.
    

---

### 3. **Facebook (Monolith + Feature Flags)**

- Facebook originally a **monolith written in PHP**.
    
- They scale it by:
    
    - Using **HHVM (custom PHP runtime)** for faster execution.
        
    - **Feature flags** â†’ new features are rolled out gradually (load tested at 2xâ€“3x expected).
        
    - **Horizontal scaling**: thousands of web servers, each handling a small fraction of requests.
        
- Lesson: You can keep monolith codebase but scale infra + developer tools.
    

---

### 4. **LinkedIn (Microservices + Messaging)**

- LinkedIn runs on **microservices** because:
    
    - Messaging, job search, feed, ads = very different workloads.
        
- They do **soak testing** (run systems for days under load) to ensure:
    
    - No memory leaks.
        
    - Kafka message queues donâ€™t get stuck.
        
- Lesson: For long-running, high-volume features like messaging, **soak testing** is essential.
    

---

### 5. **Microsoft (Office 365, Teams)**

- Office 365 & Teams handle **millions of concurrent users**.
    
- They use:
    
    - **Load testing labs** (simulate corporate traffic).
        
    - **Geo-distributed scaling** (servers across regions).
        
    - **Warm instances** â†’ Teams meetings spin up instantly because servers are pre-warmed.
        
- Lesson: Warm instances = essential for user-facing â€œinstantâ€ experiences.
    

---

### 6. **E-commerce (Amazon, Flipkart)**

- Flash sales = classic **monolith scaling issue**.
    
- Problems:
    
    - Checkout system crashes.
        
    - Payment gateway bottlenecks.
        
    - Inventory DB deadlocks.
        
- Fixes:
    
    - Move to **microservices** (separate inventory, payment, checkout).
        
    - Use **caching (Redis, CDN)**.
        
    - **Spike testing** before sales.
        
- Lesson: Monolith often fails under **spiky traffic** â†’ microservices + caching help.
    

---

### ğŸ”‘ Connectivity / Final Takeaway

- **Monoliths** work if workload is predictable + infra is optimized (Stack Overflow, Facebook early).
    
- **Microservices** help when complexity grows (Uber, LinkedIn, E-commerce).
    
- **Monorepo/Turborepo** help teams coordinate in both monolith + microservices worlds.
    
- **Scaling (vertical + horizontal, warm instances)** keeps systems alive under sudden surges.
    
- **DB optimization** is always key (indexes, queries, caching).
    
- **Load testing** is the glue â†’ ensures you know your limits before real users arrive.



# ğŸ§‘â€ğŸ’» Case Study 1: **CodeAgon (Coding Contest with 100K concurrent users)**

### ğŸ“ Problem

- Single-day coding contest â†’ **sudden spike of 100K+ concurrent users**.
    
- Monolith app (users â†’ contest problems â†’ submissions â†’ evaluation).
    
- Needs **<150ms submission response time**.
    

---

### âš¡ Challenges

1. **Traffic spikes** â†’ everyone logs in at the same time.
    
2. **Submissions** = heavy DB writes (100K users submitting every few seconds).
    
3. **Evaluation service** = CPU-heavy (running code).
    
4. **Leaderboard** = real-time updates.
    

---

### ğŸ› ï¸ Fixes

- **App Layer (Monolith Optimization)**
    
    - Keep monolith, but **scale horizontally** with **load balancer** (multiple app servers).
        
    - Use **warm instances** â†’ pre-boot app servers before contest starts.
        
- **Database Layer**
    
    - Add **indexes** on `user_id`, `problem_id`, `submission_time`.
        
    - Use **write-ahead log tuning** (faster inserts).
        
    - Store raw code submissions in **object storage (S3, GCS)** instead of DB.
        
    - Leaderboard queries served via **Redis cache**, not DB.
        
- **Evaluation Layer**
    
    - Move out of monolith â†’ microservice (isolated).
        
    - Use **Docker containers / sandboxes** to execute code safely.
        
    - Auto-scale compute nodes (warm pool ready).
        
- **Load Testing**
    
    - Simulate 200K users â†’ test DB write load & submission latency.
        
    - Stress test leaderboard under peak traffic.
        

---

### âœ… Result

- Users get **sub-150ms submission latency**.
    
- System handles 100K+ concurrent users with **no downtime**.
    
- Monolith still used, but **with microservices for evaluation**.


# ğŸ›’ Case Study 2: **Flipkart Sale Day (E-commerce Flash Sale)**

### ğŸ“ Problem

- Millions of users logging in **at the same second** for flash deals.
    
- Monolith architecture struggles â†’ checkout & payments crash.
    

---

### âš¡ Challenges

1. **Login surge** â†’ authentication service bottleneck.
    
2. **Checkout race conditions** â†’ multiple users trying same item.
    
3. **Payment gateway overload**.
    
4. **Inventory DB** deadlocks (stock count updates).
    

---

### ğŸ› ï¸ Fixes

- **App Layer**
    
    - Move to **microservices**:
        
        - Auth service
            
        - Cart service
            
        - Payment service
            
        - Inventory service
            
    - **Queue-based checkout**: put users in a virtual queue.
        
- **Database Layer**
    
    - **Sharding** inventory DB by product ID.
        
    - **Indexes** on hot fields (`product_id`, `user_id`).
        
    - Use **event sourcing** (writes to log â†’ DB updated asynchronously).
        
- **Caching**
    
    - **Redis for cart & inventory** (read hits served from cache).
        
    - **CDN for product pages** (no DB hits for static data).
        
- **Scaling**
    
    - **Warm instances** spun up hours before sale.
        
    - **Horizontal scaling** across multiple regions (closer to users).
        
- **Load Testing**
    
    - Simulate sale-day traffic (10x expected).
        
    - **Spike testing**: 1M users hitting checkout at once.
        

---

### âœ… Result

- Smooth checkout with **no crashes**.
    
- Payments distributed across multiple gateways.
    
- Inventory consistency maintained (no overselling).
    

---

# ğŸ”— Connectivity Between the Two

- **Both cases face â€œtraffic spikesâ€** â†’ warm instances, horizontal scaling.
    
- **Both need DB optimization** â†’ indexes, caching, separating hot paths.
    
- **Both benefit from microservices** for heavy workloads (evaluation for CodeAgon, checkout for Flipkart).
    
- **Both require load testing** before event â†’ simulate peak traffic.
    
- **Difference**:
    
    - CodeAgon â†’ CPU-heavy (evaluation).
        
    - Flipkart â†’ IO-heavy (payments, DB writes).
        

---

ğŸ‘‰ In short:

- **CodeAgon** teaches you how to scale a **read/write-heavy contest monolith** for short peak load.
    
- **Flipkart** teaches you how to scale a **transaction-heavy app** where money & inventory consistency matter.

![image-708.png](../../Images/image-708.png)


![image-709.png](../../Images/image-709.png)

## ğŸš€ Shopifyâ€™s Monolith Scaling Story

### 1. **Big Hardware First (Vertical Scaling)**

- Early days: they just ran the Ruby on Rails monolith on bigger and bigger servers.
    
- More RAM, more CPU â†’ worked fine until traffic exploded.
    

---

### 2. **Horizontal Scaling with Load Balancers**

- Instead of **one huge monolith**, they ran **many copies** of the monolith.
    
- A load balancer (NGINX/HAProxy) distributed incoming web requests.
    
- To make this work, they made the app **stateless** (sessions moved to Redis/Memcached).
    

---

### 3. **Database Scaling**

- The **database was the biggest bottleneck**.
    
- They used:
    
    - **Connection pooling** (so thousands of app requests donâ€™t overwhelm DB).
        
    - **Read replicas** â†’ writes go to primary DB, reads spread to replicas.
        
    - **Sharding by shop_id** â†’ each merchantâ€™s data lives in its own shard so DB load is split.
        
    - **Caching with Memcached/Redis** to reduce DB load.
        

---

### 4. **Async Queues for Heavy Work**

- Sending email, generating invoices, image processing â†’ all pushed to **job queues (Sidekiq/Resque)**.
    
- Keeps the main monolith fast.
    

---

### 5. **CDN & Edge Caching**

- Static assets (images, CSS, JS) moved to **CDNs** (Cloudflare/Fastly).
    
- Pages and API responses cached at the edge.
    

---

### 6. **Feature Flags + Modularization**

- Even though one big monolith, Shopify made it **modular inside**.
    
- Different teams own different parts (auth, checkout, orders).
    
- Feature flags let them turn things on/off without redeploying the whole monolith.
    

---

### 7. **Specialized Services, but Still Monolith Core**

- Over time, they added **special services** (e.g., search, billing engine).
    
- But the **core logic (checkout, products, orders)** is still inside the Rails monolith.
    
- They call this approach **â€œmodular monolithâ€** instead of microservices.
    

---

## âš–ï¸ The Tradeoff

- âœ… Easier to develop (one repo, one codebase, no crazy distributed systems).
    
- âœ… Easier for product teams to work fast.
    
- âœ… Scaling solved with **infra + database + caching**.
    
- âŒ Still, you canâ€™t scale **checkout** separately from **orders** â€” the whole monolith scales together.
    
- âŒ As teams grow, coordination becomes harder.


## ğŸ— Amazonâ€™s Monolith (early 2000s)

- Everything (product catalog, search, payments, reviews, checkout) was in **one huge monolithic application**.
    
- It ran on a **single codebase + single database**.
    
- Worked fine when Amazon was small, but then ğŸ‘‡
    

---

## ğŸš¨ Scaling Problems

1. **Scaling the team was impossible**
    
    - Thousands of developers working in the same repo.
        
    - Changing checkout might break search.
        
    - Deployments were rare and risky â†’ â€œbig bangâ€ deploys.
        
2. **Scaling the system**
    
    - On peak days (like Christmas), search traffic would spike.
        
    - But they had to scale the **whole monolith**, not just search.
        
    - Wasted infra and made deployments slower.
        
3. **Database bottlenecks**
    
    - All features hit the **same giant Oracle database**.
        
    - Locks and contention killed performance.
        

---

## ğŸ’¡ How They Scaled the Monolith (before microservices)

- **Vertical scaling** â†’ bigger machines, faster DB servers.
    
- **Horizontal scaling** â†’ multiple monolith instances behind load balancers.
    
- **Database replication** â†’ read replicas for scaling reads.
    
- **Caching layers** (Akamai CDN, in-memory caches).
    

This worked for a while, but Amazon was **growing too fast**.

---

## ğŸ”„ The Shift to Microservices (early 2000s)

Jeff Bezos famously gave the **â€œAPI Mandateâ€**:

- Every team must expose their data and functionality **only through service interfaces (APIs)**.
    
- No direct DB calls across teams.
    
- Teams own their service end-to-end (code + DB + scaling).
    

So:

- **Catalog became a service**.
    
- **Search became a service**.
    
- **Payments became a service**.
    
- **Recommendations became a service**.
    

Now, each team could:

- Scale **only their part** independently.
    
- Deploy faster without waiting for the whole company.
    
- Experiment without breaking others.
    

---

## âš–ï¸ Lessons from Amazon

- The **monolith scaled technically** (caching, DB scaling, replication).
    
- But the **organization didnâ€™t scale** (too many devs stepping on each otherâ€™s toes).
    
- Moving to microservices solved **team autonomy + scaling bottlenecks**.
    
- The side effect? Amazon accidentally invented what became **AWS** (they built infra for microservices, then sold it).